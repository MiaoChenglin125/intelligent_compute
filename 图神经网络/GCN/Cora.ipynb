{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<frozen importlib._bootstrap>:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n"
     ]
    }
   ],
   "source": [
    "import os.path\n",
    "import pickle\n",
    "from collections import namedtuple\n",
    "import itertools\n",
    "import numpy as np\n",
    "import scipy.sparse as sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义一个namedtuple类型Data，并包含[]属性\n",
    "#数据集已经下载好，未导入pyG\n",
    "Data=namedtuple(\"Data\",['x','y','adjacency','train_mask','val_mask','test_mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CoraData(object):\n",
    "    def __init__(self,data_root='../dataset/cora',rebuild=False):\n",
    "        self.data_root=data_root\n",
    "        self.filenames=[\n",
    "            \"ind.cora.{}\".format(name) for name in [\n",
    "                'x','tx','allx','y','ty','ally','graph','test.index'\n",
    "            ]\n",
    "        ]\n",
    "        # ind.dataset_str.x => 训练实例的特征向量，是scipy.sparse.csr.csr_matrix类对象，shape:(140, 1433)\n",
    "        # ind.dataset_str.tx => 测试实例的特征向量,shape:(1000, 1433)\n",
    "        # ind.dataset_str.allx => 有标签的+无无标签训练实例的特征向量，是ind.dataset_str.x的超集，shape:(1708, 1433)\n",
    "        # ind.dataset_str.y => 训练实例的标签，独热编码，numpy.ndarray类的实例，是numpy.ndarray对象，shape：(140, 7)\n",
    "        # ind.dataset_str.ty => 测试实例的标签，独热编码，numpy.ndarray类的实例,shape:(1000, 7)\n",
    "        # ind.dataset_str.ally => 对应于ind.dataset_str.allx的标签，独热编码,shape:(1708, 7)\n",
    "        # ind.dataset_str.graph => 图数据，collections.defaultdict类的实例，格式为 {index：[index_of_neighbor_nodes]}\n",
    "        # ind.dataset_str.test.index => 测试实例的id，2157行\n",
    "        # 上述文件必须都用python的pickle模块存储\n",
    "\n",
    "        save_file=os.path.join(self.data_root,\"processed_cora.pkl\")\n",
    "        if os.path.exists(save_file) and not rebuild:\n",
    "            print(\"Using Cached file:{}\".format(save_file))\n",
    "            #拆封数据-读档\n",
    "            self._data=pickle.load(open(save_file,\"rb\"))\n",
    "        else:\n",
    "            self._data=self.process_data()\n",
    "            with open(save_file,\"wb\") as f:\n",
    "                #封装数据-存档\n",
    "                pickle.dump(self.data,f)\n",
    "            print(\"Cached file:{}\".format(save_file))\n",
    "    def data(self):\n",
    "        return self._data\n",
    "    def process_data(self):\n",
    "        print(\"Processing data ...\")\n",
    "        x,tx,allx,y,ty,ally,graph,test_index=[\n",
    "            self.read_data(os.path.join(self.data_root,\"raw\",name)) for name in self.filenames\n",
    "        ]\n",
    "        train_index=np.arange(y.shape[0])\n",
    "        val_index=np.arange(y.shape[0],y.shape[0]+500)\n",
    "        sorted_test_index=sorted(test_index)\n",
    "        x=np.concatenate((allx,tx),axis=0)\n",
    "        y=np.concatenate((ally,ty),axis=0).argmax(axis=1)\n",
    "        #测试节点排序？\n",
    "        x[test_index]=x[sorted_test_index]\n",
    "        y[test_index]=y[sorted_test_index]\n",
    "        num_nodes=x.shape[0]\n",
    "        \n",
    "        train_mask=np.zeros(num_nodes,dtype=np.bool)\n",
    "        val_mask=np.zeros(num_nodes,dtype=np.bool)\n",
    "        test_mask=np.zeros(num_nodes,dtype=np.bool)\n",
    "        train_mask[train_index]=True\n",
    "        val_mask[val_index]=True\n",
    "        test_mask[test_index]=True\n",
    "        \n",
    "        adjacency=self.build_adjacency(graph)\n",
    "        print(\"Node's feature shape: \", x.shape)\n",
    "        print(\"Node's label shape: \", y.shape)\n",
    "        print(\"Adjacency's shape: \", adjacency.shape)\n",
    "        print(\"Number of training nodes: \", train_mask.sum())\n",
    "        print(\"Number of validation nodes: \", val_mask.sum())\n",
    "        print(\"Number of test nodes: \", test_mask.sum())\n",
    "        \n",
    "        return Data(x=x,y=y,adjacency=adjacency,train_mask=train_mask,val_mask=val_mask,test_mask=test_mask)\n",
    "    \n",
    "    def build_adjacency(self,adj_dict):\n",
    "        #根据邻接表创建邻接矩阵\n",
    "        print(\"adj_dict\",adj_dict)\n",
    "        edge_index=[]\n",
    "        num_nodes=len(adj_dict)\n",
    "        for src,dst in adj_dict.items():\n",
    "            edge_index.extend([src,v] for v in dst)\n",
    "            edge_index.extend([v,src] for v in dst)\n",
    "        #去除列表中的重复元素\n",
    "        print(\"edge_index\",edge_index)\n",
    "        edge_index=list(k for k,_ in itertools.groupby(sorted(edge_index)))\n",
    "        edge_index=np.asarray(edge_index)\n",
    "        print(\"edge_index\",edge_index)\n",
    "        #以稀疏矩阵方式存储和运算\n",
    "        adjacency=sp.coo_matrix((np.ones(len(edge_index),edge_index[:,0],edge_index[:,1])),shape=(num_nodes,num_nodes),dtype=\"float32\")\n",
    "        return adjacency\n",
    "    \n",
    "    def read_data(path):\n",
    "        name=os.path.basename(path)\n",
    "        if name==\"ind.cora.test.index\":\n",
    "            out=np.genfromtxt(path,dtype=\"int64\")\n",
    "        else:\n",
    "            out=pickle.load(open(path,\"rb\"),encoding=\"latin1\")\n",
    "            out=out.toarray() if hasattr(out,\"toarray\") else out\n",
    "        return out\n",
    "    \n",
    "    def normalization(adjacency):\n",
    "        #计算拉普拉斯矩阵\n",
    "        adjacency+=sp.eye(adjacency.shape[0])#增加自连接\n",
    "        degree=np.array(adjacency.sum(axis=1))\n",
    "        d_hat=sp.diags(np.power(degree,-0.5).flatten())\n",
    "        return d_hat.dot(adjacency).dot(d_hat).tocoo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 2 1 3]\n"
     ]
    }
   ],
   "source": [
    "y1=np.array([[1,0,0,0],[0,0,1,0]])\n",
    "y2=np.array([[0,1,0,0],[0,0,0,1]])\n",
    "y=np.concatenate([y1,y2],axis=0)\n",
    "y=y.argmax(axis=1)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
